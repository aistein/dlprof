{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from utils import download, extract, get_cifar10_batches, get_cifar10_test_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_10_download = Path(\"data/cifar-10-python.tar.gz\")\n",
    "if not cifar_10_download.exists():\n",
    "    download(\"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\", cifar_10_download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_loc = Path(\"data/cifar-10-batches-py\")\n",
    "if not extract_loc.exists():\n",
    "    extract(cifar_10_download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = get_cifar10_batches()\n",
    "test_batch = get_cifar10_test_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "batch_size = 64\n",
    "n = 32\n",
    "n_channels = 3\n",
    "n_samples = batches.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = tf.placeholder(tf.float32, [batch_size, n, n, n_channels])\n",
    "category = tf.placeholder(tf.int32, [batch_size, 1])\n",
    "conv1 = tf.layers.conv2d(\n",
    "    inputs=inp,\n",
    "    filters=32,\n",
    "    kernel_size=[5, 5],\n",
    "    padding=\"same\",\n",
    "    activation=tf.nn.relu)\n",
    "\n",
    "pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
    "\n",
    "# Convolutional Layer #2 and Pooling Layer #2\n",
    "conv2 = tf.layers.conv2d(\n",
    "    inputs=pool1,\n",
    "    filters=64,\n",
    "    kernel_size=[5, 5],\n",
    "    padding=\"same\",\n",
    "    activation=tf.nn.relu)\n",
    "pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
    "\n",
    "# Dense Layer\n",
    "pool2_flat = tf.reshape(pool2, [-1, 8 * 8 * 64])\n",
    "dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n",
    "dropout = tf.layers.dropout(inputs=dense, rate=0.4)\n",
    "\n",
    "# Logits Layer\n",
    "logits = tf.layers.dense(inputs=dropout, units=10)\n",
    "\n",
    "loss = tf.losses.sparse_softmax_cross_entropy(labels=category, logits=logits)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "train_op = optimizer.minimize(\n",
    "    loss=loss,\n",
    "    global_step=tf.train.get_global_step())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0, train loss: 61.58, test loss: [941.1078]\n",
      "iter: 1, train loss: 820.25, test loss: [30.243958]\n",
      "iter: 2, train loss: 43.11, test loss: [58.493683]\n",
      "iter: 3, train loss: 55.17, test loss: [3.3320942]\n",
      "iter: 4, train loss: 2.86, test loss: [2.842989]\n",
      "iter: 5, train loss: 2.35, test loss: [2.3068764]\n",
      "iter: 6, train loss: 2.31, test loss: [2.337867]\n",
      "iter: 7, train loss: 2.40, test loss: [2.7994213]\n",
      "iter: 8, train loss: 2.77, test loss: [2.4857194]\n",
      "iter: 9, train loss: 4.79, test loss: [2.3434677]\n",
      "iter: 10, train loss: 2.29, test loss: [2.2797074]\n",
      "iter: 11, train loss: 2.34, test loss: [2.3240404]\n",
      "iter: 12, train loss: 2.29, test loss: [2.3757832]\n",
      "iter: 13, train loss: 2.83, test loss: [2.3269143]\n",
      "iter: 14, train loss: 2.30, test loss: [2.2861545]\n",
      "iter: 15, train loss: 2.39, test loss: [2.3739185]\n",
      "iter: 16, train loss: 2.30, test loss: [2.4947348]\n",
      "iter: 17, train loss: 2.57, test loss: [2.307457]\n",
      "iter: 18, train loss: 2.98, test loss: [2.3006496]\n",
      "iter: 19, train loss: 2.34, test loss: [2.2866554]\n",
      "iter: 20, train loss: 2.32, test loss: [3.7622643]\n",
      "iter: 21, train loss: 2.27, test loss: [2.2779334]\n",
      "iter: 22, train loss: 2.30, test loss: [2.3037906]\n",
      "iter: 23, train loss: 2.40, test loss: [2.2997363]\n",
      "iter: 24, train loss: 2.26, test loss: [2.3069696]\n",
      "iter: 25, train loss: 2.30, test loss: [2.2984102]\n",
      "iter: 26, train loss: 2.38, test loss: [3.067237]\n",
      "iter: 27, train loss: 2.29, test loss: [2.3118262]\n",
      "iter: 28, train loss: 2.31, test loss: [2.2976263]\n",
      "iter: 29, train loss: 2.37, test loss: [2.315454]\n",
      "iter: 30, train loss: 2.29, test loss: [2.4591393]\n",
      "iter: 31, train loss: 3.64, test loss: [2.319097]\n",
      "iter: 32, train loss: 2.35, test loss: [2.3065941]\n",
      "iter: 33, train loss: 2.34, test loss: [2.3483505]\n",
      "iter: 34, train loss: 2.30, test loss: [2.3027377]\n",
      "iter: 35, train loss: 2.28, test loss: [2.3005445]\n",
      "iter: 36, train loss: 2.40, test loss: [2.3120966]\n",
      "iter: 37, train loss: 2.37, test loss: [2.2859461]\n",
      "iter: 38, train loss: 2.34, test loss: [2.290095]\n",
      "iter: 39, train loss: 3.67, test loss: [2.301358]\n",
      "iter: 40, train loss: 2.30, test loss: [2.375817]\n",
      "iter: 41, train loss: 2.34, test loss: [2.32128]\n",
      "iter: 42, train loss: 2.31, test loss: [2.3011322]\n",
      "iter: 43, train loss: 2.34, test loss: [2.3025517]\n",
      "iter: 44, train loss: 2.32, test loss: [2.291382]\n",
      "iter: 45, train loss: 2.31, test loss: [2.3257065]\n",
      "iter: 46, train loss: 2.30, test loss: [2.3429475]\n",
      "iter: 47, train loss: 2.30, test loss: [2.2967658]\n",
      "iter: 48, train loss: 2.31, test loss: [2.289339]\n",
      "iter: 49, train loss: 2.30, test loss: [2.298036]\n",
      "iter: 50, train loss: 2.46, test loss: [2.4752083]\n",
      "iter: 51, train loss: 2.37, test loss: [2.2989936]\n",
      "iter: 52, train loss: 2.34, test loss: [2.2985024]\n",
      "iter: 53, train loss: 2.33, test loss: [2.2920551]\n",
      "iter: 54, train loss: 2.30, test loss: [2.314795]\n",
      "iter: 55, train loss: 2.30, test loss: [2.5283794]\n",
      "iter: 56, train loss: 2.30, test loss: [2.3030307]\n",
      "iter: 57, train loss: 2.31, test loss: [2.3211207]\n",
      "iter: 58, train loss: 2.69, test loss: [2.303835]\n",
      "iter: 59, train loss: 2.28, test loss: [2.2875404]\n",
      "iter: 60, train loss: 2.30, test loss: [2.3012676]\n",
      "iter: 61, train loss: 2.30, test loss: [2.3061543]\n",
      "iter: 62, train loss: 2.30, test loss: [2.4911585]\n",
      "iter: 63, train loss: 2.37, test loss: [2.3117929]\n",
      "iter: 64, train loss: 2.30, test loss: [2.2984807]\n",
      "iter: 65, train loss: 2.30, test loss: [2.2925746]\n",
      "iter: 66, train loss: 2.32, test loss: [2.3079772]\n",
      "iter: 67, train loss: 2.31, test loss: [2.297623]\n",
      "iter: 68, train loss: 2.44, test loss: [2.3046145]\n",
      "iter: 69, train loss: 2.31, test loss: [2.2997923]\n",
      "iter: 70, train loss: 2.30, test loss: [2.3018174]\n",
      "iter: 71, train loss: 2.33, test loss: [2.295739]\n",
      "iter: 72, train loss: 2.60, test loss: [2.3019533]\n",
      "iter: 73, train loss: 2.35, test loss: [2.2840538]\n",
      "iter: 74, train loss: 2.31, test loss: [2.4610956]\n",
      "iter: 75, train loss: 2.31, test loss: [2.2915592]\n",
      "iter: 76, train loss: 2.30, test loss: [2.30619]\n",
      "iter: 77, train loss: 2.33, test loss: [2.3138342]\n",
      "iter: 78, train loss: 2.29, test loss: [2.2959652]\n",
      "iter: 79, train loss: 2.30, test loss: [2.3009095]\n",
      "iter: 80, train loss: 2.44, test loss: [2.3230233]\n",
      "iter: 81, train loss: 2.28, test loss: [2.2939603]\n",
      "iter: 82, train loss: 2.30, test loss: [2.303266]\n",
      "iter: 83, train loss: 2.30, test loss: [2.3001084]\n",
      "iter: 84, train loss: 2.32, test loss: [2.30165]\n",
      "iter: 85, train loss: 2.30, test loss: [2.30095]\n",
      "iter: 86, train loss: 2.39, test loss: [2.3295078]\n",
      "iter: 87, train loss: 2.30, test loss: [2.3368697]\n",
      "iter: 88, train loss: 2.30, test loss: [2.39178]\n",
      "iter: 89, train loss: 2.30, test loss: [2.356422]\n",
      "iter: 90, train loss: 2.30, test loss: [2.2924347]\n",
      "iter: 91, train loss: 2.29, test loss: [2.281866]\n",
      "iter: 92, train loss: 2.41, test loss: [2.2974725]\n",
      "iter: 93, train loss: 2.30, test loss: [2.306355]\n",
      "iter: 94, train loss: 2.31, test loss: [2.3142915]\n",
      "iter: 95, train loss: 2.28, test loss: [2.2996948]\n",
      "iter: 96, train loss: 2.30, test loss: [2.2907133]\n",
      "iter: 97, train loss: 2.31, test loss: [2.3215785]\n",
      "iter: 98, train loss: 2.30, test loss: [2.3766217]\n",
      "iter: 99, train loss: 2.30, test loss: [2.3037438]\n",
      "iter: 100, train loss: 2.30, test loss: [2.2973094]\n",
      "iter: 101, train loss: 2.30, test loss: [2.3027954]\n",
      "iter: 102, train loss: 2.34, test loss: [2.3144803]\n",
      "iter: 103, train loss: 2.31, test loss: [2.301333]\n",
      "iter: 104, train loss: 2.31, test loss: [2.3781004]\n",
      "iter: 105, train loss: 2.30, test loss: [2.3127735]\n",
      "iter: 106, train loss: 2.33, test loss: [2.545341]\n",
      "iter: 107, train loss: 2.99, test loss: [2.3025036]\n",
      "iter: 108, train loss: 2.30, test loss: [2.2991266]\n",
      "iter: 109, train loss: 2.30, test loss: [2.3149438]\n",
      "iter: 110, train loss: 2.30, test loss: [2.3013208]\n",
      "iter: 111, train loss: 2.94, test loss: [2.301868]\n",
      "iter: 112, train loss: 2.33, test loss: [2.300766]\n",
      "iter: 113, train loss: 2.31, test loss: [2.422424]\n",
      "iter: 114, train loss: 2.30, test loss: [2.439784]\n",
      "iter: 115, train loss: 2.30, test loss: [2.3022263]\n",
      "iter: 116, train loss: 2.30, test loss: [2.3027925]\n",
      "iter: 117, train loss: 2.29, test loss: [2.3197823]\n",
      "iter: 118, train loss: 2.30, test loss: [2.300036]\n",
      "iter: 119, train loss: 2.32, test loss: [2.2983952]\n",
      "iter: 120, train loss: 2.36, test loss: [2.3024302]\n",
      "iter: 121, train loss: 2.31, test loss: [2.3033466]\n",
      "iter: 122, train loss: 2.47, test loss: [2.2995758]\n",
      "iter: 123, train loss: 2.50, test loss: [2.3012576]\n",
      "iter: 124, train loss: 2.31, test loss: [2.301932]\n",
      "iter: 125, train loss: 2.31, test loss: [2.302113]\n",
      "iter: 126, train loss: 2.29, test loss: [2.298203]\n",
      "iter: 127, train loss: 2.30, test loss: [2.2910533]\n",
      "iter: 128, train loss: 2.30, test loss: [2.3303437]\n",
      "iter: 129, train loss: 2.31, test loss: [2.3491392]\n",
      "iter: 130, train loss: 2.30, test loss: [2.3259747]\n",
      "iter: 131, train loss: 2.30, test loss: [2.2851505]\n",
      "iter: 132, train loss: 2.30, test loss: [2.30551]\n",
      "iter: 133, train loss: 2.62, test loss: [2.311459]\n",
      "iter: 134, train loss: 2.30, test loss: [2.3028595]\n",
      "iter: 135, train loss: 2.30, test loss: [2.39921]\n",
      "iter: 136, train loss: 2.63, test loss: [2.301976]\n",
      "iter: 137, train loss: 2.30, test loss: [2.349711]\n",
      "iter: 138, train loss: 2.30, test loss: [2.3018467]\n",
      "iter: 139, train loss: 2.29, test loss: [2.299986]\n",
      "iter: 140, train loss: 2.29, test loss: [2.3128102]\n",
      "iter: 141, train loss: 2.30, test loss: [4.487383]\n",
      "iter: 142, train loss: 2.29, test loss: [2.3094983]\n",
      "iter: 143, train loss: 2.30, test loss: [2.3895826]\n",
      "iter: 144, train loss: 2.30, test loss: [2.3024864]\n",
      "iter: 145, train loss: 2.30, test loss: [2.301457]\n",
      "iter: 146, train loss: 2.31, test loss: [2.5102606]\n",
      "iter: 147, train loss: 2.30, test loss: [2.2977417]\n",
      "iter: 148, train loss: 2.30, test loss: [2.299556]\n",
      "iter: 149, train loss: 2.30, test loss: [2.3051105]\n",
      "iter: 150, train loss: 2.30, test loss: [2.3179932]\n",
      "iter: 151, train loss: 2.30, test loss: [2.2957723]\n",
      "iter: 152, train loss: 2.33, test loss: [2.3006046]\n",
      "iter: 153, train loss: 2.30, test loss: [2.2943354]\n",
      "iter: 154, train loss: 2.33, test loss: [2.3028452]\n",
      "iter: 155, train loss: 2.31, test loss: [2.2939334]\n",
      "iter: 156, train loss: 2.33, test loss: [2.3023512]\n",
      "iter: 157, train loss: 2.30, test loss: [2.2964475]\n",
      "iter: 158, train loss: 2.30, test loss: [2.3008437]\n",
      "iter: 159, train loss: 2.30, test loss: [2.3031282]\n",
      "iter: 160, train loss: 2.30, test loss: [2.3055224]\n",
      "iter: 161, train loss: 2.31, test loss: [2.2967987]\n",
      "iter: 162, train loss: 2.30, test loss: [2.3084135]\n",
      "iter: 163, train loss: 2.30, test loss: [2.3021212]\n",
      "iter: 164, train loss: 2.31, test loss: [2.329678]\n",
      "iter: 165, train loss: 2.30, test loss: [2.312379]\n",
      "iter: 166, train loss: 2.30, test loss: [2.3034296]\n",
      "iter: 167, train loss: 2.30, test loss: [2.3529468]\n",
      "iter: 168, train loss: 2.30, test loss: [2.3424273]\n",
      "iter: 169, train loss: 2.70, test loss: [2.2973828]\n",
      "iter: 170, train loss: 2.30, test loss: [2.3042524]\n",
      "iter: 171, train loss: 2.37, test loss: [2.3296413]\n",
      "iter: 172, train loss: 2.30, test loss: [2.2989793]\n",
      "iter: 173, train loss: 2.30, test loss: [2.2981243]\n",
      "iter: 174, train loss: 2.92, test loss: [2.3034978]\n",
      "iter: 175, train loss: 2.30, test loss: [2.284812]\n",
      "iter: 176, train loss: 2.30, test loss: [2.3026161]\n",
      "iter: 177, train loss: 2.30, test loss: [2.3010411]\n",
      "iter: 178, train loss: 2.30, test loss: [2.3432903]\n",
      "iter: 179, train loss: 2.30, test loss: [2.3103013]\n",
      "iter: 180, train loss: 2.30, test loss: [2.3011675]\n",
      "iter: 181, train loss: 2.30, test loss: [2.34122]\n",
      "iter: 182, train loss: 2.86, test loss: [2.2991667]\n",
      "iter: 183, train loss: 2.30, test loss: [2.300322]\n",
      "iter: 184, train loss: 2.30, test loss: [2.3013124]\n",
      "iter: 185, train loss: 2.32, test loss: [2.3045545]\n",
      "iter: 186, train loss: 2.30, test loss: [2.3019605]\n",
      "iter: 187, train loss: 2.30, test loss: [2.3231885]\n",
      "iter: 188, train loss: 2.30, test loss: [2.3406076]\n",
      "iter: 189, train loss: 2.30, test loss: [2.2993207]\n",
      "iter: 190, train loss: 2.32, test loss: [2.3021817]\n",
      "iter: 191, train loss: 2.29, test loss: [2.297752]\n",
      "iter: 192, train loss: 2.30, test loss: [2.3029547]\n",
      "iter: 193, train loss: 2.30, test loss: [2.3297346]\n",
      "iter: 194, train loss: 2.32, test loss: [2.2921777]\n",
      "iter: 195, train loss: 2.34, test loss: [2.302274]\n",
      "iter: 196, train loss: 2.30, test loss: [2.301395]\n",
      "iter: 197, train loss: 2.33, test loss: [2.3834538]\n",
      "iter: 198, train loss: 2.58, test loss: [2.3049302]\n",
      "iter: 199, train loss: 2.30, test loss: [2.3899379]\n",
      "iter: 200, train loss: 2.30, test loss: [2.2994037]\n",
      "iter: 201, train loss: 2.40, test loss: [2.2971115]\n",
      "iter: 202, train loss: 2.32, test loss: [2.3574305]\n",
      "iter: 203, train loss: 2.30, test loss: [2.3028002]\n",
      "iter: 204, train loss: 2.30, test loss: [2.2914617]\n",
      "iter: 205, train loss: 2.50, test loss: [2.3018923]\n",
      "iter: 206, train loss: 2.30, test loss: [2.3024406]\n",
      "iter: 207, train loss: 2.30, test loss: [2.3026202]\n",
      "iter: 208, train loss: 2.30, test loss: [2.302051]\n",
      "iter: 209, train loss: 2.30, test loss: [2.2993598]\n",
      "iter: 210, train loss: 2.30, test loss: [2.3025627]\n",
      "iter: 211, train loss: 2.91, test loss: [2.3026698]\n",
      "iter: 212, train loss: 2.31, test loss: [2.3023367]\n",
      "iter: 213, train loss: 2.30, test loss: [2.300023]\n",
      "iter: 214, train loss: 2.30, test loss: [2.2850027]\n",
      "iter: 215, train loss: 2.34, test loss: [2.3001735]\n",
      "iter: 216, train loss: 2.73, test loss: [2.302562]\n",
      "iter: 217, train loss: 2.30, test loss: [2.302998]\n",
      "iter: 218, train loss: 2.28, test loss: [2.3017452]\n",
      "iter: 219, train loss: 2.31, test loss: [2.301811]\n",
      "iter: 220, train loss: 2.30, test loss: [2.3022585]\n",
      "iter: 221, train loss: 2.34, test loss: [2.3028846]\n",
      "iter: 222, train loss: 2.30, test loss: [2.3029962]\n",
      "iter: 223, train loss: 2.30, test loss: [2.3000026]\n",
      "iter: 224, train loss: 2.30, test loss: [2.4319797]\n",
      "iter: 225, train loss: 2.30, test loss: [2.3018076]\n",
      "iter: 226, train loss: 2.50, test loss: [2.3027682]\n",
      "iter: 227, train loss: 2.29, test loss: [2.3003287]\n",
      "iter: 228, train loss: 2.45, test loss: [2.6677718]\n",
      "iter: 229, train loss: 2.30, test loss: [2.317109]\n",
      "iter: 230, train loss: 2.30, test loss: [2.2926412]\n",
      "iter: 231, train loss: 2.30, test loss: [2.2940576]\n",
      "iter: 232, train loss: 2.30, test loss: [2.305035]\n",
      "iter: 233, train loss: 2.30, test loss: [2.2973347]\n",
      "iter: 234, train loss: 2.30, test loss: [2.3017416]\n",
      "iter: 235, train loss: 2.30, test loss: [2.331597]\n",
      "iter: 236, train loss: 2.30, test loss: [3.8810487]\n",
      "iter: 237, train loss: 2.30, test loss: [2.3025248]\n",
      "iter: 238, train loss: 2.29, test loss: [2.3015904]\n",
      "iter: 239, train loss: 2.30, test loss: [2.303575]\n",
      "iter: 240, train loss: 2.29, test loss: [2.3020933]\n",
      "iter: 241, train loss: 2.34, test loss: [2.31325]\n",
      "iter: 242, train loss: 2.30, test loss: [2.4433951]\n",
      "iter: 243, train loss: 2.30, test loss: [2.3020911]\n",
      "iter: 244, train loss: 2.30, test loss: [2.3007212]\n",
      "iter: 245, train loss: 2.30, test loss: [2.3152058]\n",
      "iter: 246, train loss: 2.30, test loss: [2.3041973]\n",
      "iter: 247, train loss: 2.30, test loss: [2.3052516]\n",
      "iter: 248, train loss: 2.28, test loss: [2.2983353]\n",
      "iter: 249, train loss: 2.29, test loss: [2.3014317]\n",
      "iter: 250, train loss: 2.30, test loss: [2.301105]\n",
      "iter: 251, train loss: 2.31, test loss: [2.301467]\n",
      "iter: 252, train loss: 2.30, test loss: [2.3186998]\n",
      "iter: 253, train loss: 2.30, test loss: [2.302478]\n",
      "iter: 254, train loss: 2.31, test loss: [2.3023405]\n",
      "iter: 255, train loss: 2.31, test loss: [2.360381]\n",
      "iter: 256, train loss: 2.30, test loss: [2.2976136]\n",
      "iter: 257, train loss: 2.30, test loss: [2.302092]\n",
      "iter: 258, train loss: 2.30, test loss: [2.3083797]\n",
      "iter: 259, train loss: 2.31, test loss: [2.3025396]\n",
      "iter: 260, train loss: 2.30, test loss: [2.3026233]\n",
      "iter: 261, train loss: 2.30, test loss: [2.3004355]\n",
      "iter: 262, train loss: 2.30, test loss: [2.3026092]\n",
      "iter: 263, train loss: 2.30, test loss: [2.3307838]\n",
      "iter: 264, train loss: 2.30, test loss: [2.2936873]\n",
      "iter: 265, train loss: 2.30, test loss: [2.3054478]\n",
      "iter: 266, train loss: 2.30, test loss: [2.6696353]\n",
      "iter: 267, train loss: 2.41, test loss: [2.302053]\n",
      "iter: 268, train loss: 2.29, test loss: [2.3126185]\n",
      "iter: 269, train loss: 2.30, test loss: [2.3250325]\n",
      "iter: 270, train loss: 2.30, test loss: [2.2946851]\n",
      "iter: 271, train loss: 2.30, test loss: [2.30266]\n",
      "iter: 272, train loss: 2.30, test loss: [2.3026686]\n",
      "iter: 273, train loss: 2.30, test loss: [2.3023603]\n",
      "iter: 274, train loss: 2.30, test loss: [2.3026896]\n",
      "iter: 275, train loss: 2.30, test loss: [2.301857]\n",
      "iter: 276, train loss: 2.30, test loss: [2.3021646]\n",
      "iter: 277, train loss: 2.30, test loss: [2.329598]\n",
      "iter: 278, train loss: 2.30, test loss: [2.3028045]\n",
      "iter: 279, train loss: 2.30, test loss: [2.3022752]\n",
      "iter: 280, train loss: 2.30, test loss: [2.302181]\n",
      "iter: 281, train loss: 2.30, test loss: [2.3027554]\n",
      "iter: 282, train loss: 2.30, test loss: [2.301538]\n",
      "iter: 283, train loss: 2.30, test loss: [2.3012502]\n",
      "iter: 284, train loss: 2.30, test loss: [2.3018315]\n",
      "iter: 285, train loss: 2.30, test loss: [2.3140721]\n",
      "iter: 286, train loss: 2.32, test loss: [2.302294]\n",
      "iter: 287, train loss: 2.30, test loss: [2.2957788]\n",
      "iter: 288, train loss: 2.30, test loss: [2.3022652]\n",
      "iter: 289, train loss: 2.30, test loss: [2.2975402]\n",
      "iter: 290, train loss: 2.30, test loss: [2.3026047]\n",
      "iter: 291, train loss: 2.30, test loss: [2.298436]\n",
      "iter: 292, train loss: 2.39, test loss: [2.301958]\n",
      "iter: 293, train loss: 2.30, test loss: [2.2919564]\n",
      "iter: 294, train loss: 2.30, test loss: [2.3013976]\n",
      "iter: 295, train loss: 2.30, test loss: [2.3233376]\n",
      "iter: 296, train loss: 2.29, test loss: [2.3028169]\n",
      "iter: 297, train loss: 2.40, test loss: [2.3016078]\n",
      "iter: 298, train loss: 2.29, test loss: [2.2997146]\n",
      "iter: 299, train loss: 2.30, test loss: [2.3026335]\n",
      "iter: 300, train loss: 2.30, test loss: [2.300994]\n",
      "iter: 301, train loss: 2.30, test loss: [2.3028224]\n",
      "iter: 302, train loss: 2.31, test loss: [2.3024976]\n",
      "iter: 303, train loss: 2.44, test loss: [2.3055418]\n",
      "iter: 304, train loss: 2.31, test loss: [2.3025393]\n",
      "iter: 305, train loss: 2.30, test loss: [2.3221684]\n",
      "iter: 306, train loss: 2.30, test loss: [2.3022466]\n",
      "iter: 307, train loss: 2.30, test loss: [2.2960963]\n",
      "iter: 308, train loss: 2.30, test loss: [2.43212]\n",
      "iter: 309, train loss: 2.33, test loss: [2.301333]\n",
      "iter: 310, train loss: 2.31, test loss: [2.3028948]\n",
      "iter: 311, train loss: 2.38, test loss: [2.3086665]\n",
      "iter: 312, train loss: 2.40, test loss: [2.3021693]\n",
      "iter: 313, train loss: 2.30, test loss: [2.305928]\n",
      "iter: 314, train loss: 2.30, test loss: [2.3034701]\n",
      "iter: 315, train loss: 2.30, test loss: [2.3320537]\n",
      "iter: 316, train loss: 2.30, test loss: [2.3015468]\n",
      "iter: 317, train loss: 2.30, test loss: [2.302532]\n",
      "iter: 318, train loss: 2.30, test loss: [2.3174725]\n",
      "iter: 319, train loss: 2.30, test loss: [2.3030477]\n",
      "iter: 320, train loss: 2.30, test loss: [2.3071747]\n",
      "iter: 321, train loss: 2.30, test loss: [2.3022044]\n",
      "iter: 322, train loss: 2.31, test loss: [2.303341]\n",
      "iter: 323, train loss: 2.31, test loss: [2.313102]\n",
      "iter: 324, train loss: 2.30, test loss: [2.302134]\n",
      "iter: 325, train loss: 2.30, test loss: [2.3023417]\n",
      "iter: 326, train loss: 2.30, test loss: [2.3012948]\n",
      "iter: 327, train loss: 2.29, test loss: [2.3006592]\n",
      "iter: 328, train loss: 2.30, test loss: [2.3131678]\n",
      "iter: 329, train loss: 2.30, test loss: [2.3017554]\n",
      "iter: 330, train loss: 2.30, test loss: [2.2917619]\n",
      "iter: 331, train loss: 2.30, test loss: [2.300863]\n",
      "iter: 332, train loss: 2.30, test loss: [2.3072348]\n",
      "iter: 333, train loss: 2.30, test loss: [2.3167331]\n",
      "iter: 334, train loss: 2.59, test loss: [2.3130083]\n",
      "iter: 335, train loss: 2.30, test loss: [2.3024435]\n",
      "iter: 336, train loss: 2.29, test loss: [2.3023567]\n",
      "iter: 337, train loss: 2.30, test loss: [2.283114]\n",
      "iter: 338, train loss: 2.30, test loss: [2.3011708]\n",
      "iter: 339, train loss: 2.30, test loss: [2.2987862]\n",
      "iter: 340, train loss: 2.34, test loss: [2.290953]\n",
      "iter: 341, train loss: 2.30, test loss: [2.3027704]\n",
      "iter: 342, train loss: 2.42, test loss: [2.3164644]\n",
      "iter: 343, train loss: 2.30, test loss: [2.3024974]\n",
      "iter: 344, train loss: 2.30, test loss: [2.3714278]\n",
      "iter: 345, train loss: 2.30, test loss: [2.3056555]\n",
      "iter: 346, train loss: 2.30, test loss: [2.3024585]\n",
      "iter: 347, train loss: 2.30, test loss: [2.3030424]\n",
      "iter: 348, train loss: 2.32, test loss: [2.3026667]\n",
      "iter: 349, train loss: 2.30, test loss: [2.298018]\n",
      "iter: 350, train loss: 2.30, test loss: [2.3027568]\n",
      "iter: 351, train loss: 2.30, test loss: [2.3032336]\n",
      "iter: 352, train loss: 2.30, test loss: [2.2953439]\n",
      "iter: 353, train loss: 2.29, test loss: [2.2961721]\n",
      "iter: 354, train loss: 2.30, test loss: [2.3018918]\n",
      "iter: 355, train loss: 2.30, test loss: [2.3013783]\n",
      "iter: 356, train loss: 2.30, test loss: [2.2868824]\n",
      "iter: 357, train loss: 2.30, test loss: [2.3587275]\n",
      "iter: 358, train loss: 2.30, test loss: [2.2873807]\n",
      "iter: 359, train loss: 2.30, test loss: [2.3020358]\n",
      "iter: 360, train loss: 2.30, test loss: [2.3014967]\n",
      "iter: 361, train loss: 2.30, test loss: [2.3005466]\n",
      "iter: 362, train loss: 2.30, test loss: [2.302575]\n",
      "iter: 363, train loss: 2.30, test loss: [2.3467047]\n",
      "iter: 364, train loss: 2.30, test loss: [2.3026311]\n",
      "iter: 365, train loss: 2.30, test loss: [2.2990654]\n",
      "iter: 366, train loss: 2.30, test loss: [2.2990727]\n",
      "iter: 367, train loss: 2.30, test loss: [2.300367]\n",
      "iter: 368, train loss: 2.32, test loss: [2.3026414]\n",
      "iter: 369, train loss: 2.30, test loss: [2.3028183]\n",
      "iter: 370, train loss: 2.30, test loss: [2.3005702]\n",
      "iter: 371, train loss: 2.30, test loss: [2.3334546]\n",
      "iter: 372, train loss: 2.30, test loss: [2.302586]\n",
      "iter: 373, train loss: 2.30, test loss: [2.3019283]\n",
      "iter: 374, train loss: 2.30, test loss: [2.2974598]\n",
      "iter: 375, train loss: 2.30, test loss: [2.3001466]\n",
      "iter: 376, train loss: 2.30, test loss: [2.6039696]\n",
      "iter: 377, train loss: 2.30, test loss: [2.30156]\n",
      "iter: 378, train loss: 2.29, test loss: [2.29637]\n",
      "iter: 379, train loss: 2.30, test loss: [2.2865863]\n",
      "iter: 380, train loss: 2.49, test loss: [2.298826]\n",
      "iter: 381, train loss: 2.30, test loss: [2.3025436]\n",
      "iter: 382, train loss: 2.30, test loss: [2.335972]\n",
      "iter: 383, train loss: 2.30, test loss: [2.2993662]\n",
      "iter: 384, train loss: 2.30, test loss: [2.3025904]\n",
      "iter: 385, train loss: 2.30, test loss: [2.3030405]\n",
      "iter: 386, train loss: 2.28, test loss: [2.3356457]\n",
      "iter: 387, train loss: 2.30, test loss: [2.2957406]\n",
      "iter: 388, train loss: 2.30, test loss: [2.3028688]\n",
      "iter: 389, train loss: 2.30, test loss: [2.2956672]\n",
      "iter: 390, train loss: 2.43, test loss: [2.3026428]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for ite in range(n_samples // batch_size):\n",
    "        batch = batches.sample(n=batch_size)\n",
    "        x, y = batch[1], batch[0]\n",
    "        l, _ = sess.run([loss, train_op], feed_dict={inp: np.stack(list(x)).reshape([batch_size, n, n, n_channels]), category: np.stack(list(y)).reshape([batch_size, 1])})\n",
    "        this_test_batch = test_batch.sample(n=batch_size)\n",
    "        test_x, test_y = this_test_batch[1], this_test_batch[0]\n",
    "        t_l = sess.run([loss], feed_dict={inp: np.stack(list(test_x)).reshape([batch_size, n, n, n_channels]), category: np.stack(list(test_y)).reshape([batch_size, 1])})\n",
    "        print(\"iter: {}, train loss: {:.2f}, test loss: {}\".format(ite, l, t_l))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
