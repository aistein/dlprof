{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import tensorflow as tf\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode):\n",
    "    emb_size = 50\n",
    "    filters=10\n",
    "    kernel_size=3\n",
    "    \n",
    "    with open(\"data/dictionary.pkl\", \"rb\") as f:\n",
    "        dictionary = pkl.load(f)\n",
    "    \n",
    "    values = list(range(len(dictionary)))\n",
    "    keys = list(dictionary)\n",
    "    \n",
    "    table = tf.contrib.lookup.HashTable(\n",
    "      tf.contrib.lookup.KeyValueTensorInitializer(keys, values), -1\n",
    "    )\n",
    "    \n",
    "    word_embeddings = tf.get_variable(\n",
    "        \"word_embeddings\",\n",
    "        shape=[len(dictionary), emb_size]\n",
    "    )\n",
    "    \n",
    "    u_inputs = features[\"user_input\"]\n",
    "    i_inputs = features[\"item_input\"]\n",
    "    \n",
    "    u_inputs = table.lookup(u_inputs)\n",
    "    i_inputs = table.lookup(i_inputs)\n",
    "\n",
    "    u_inputs = tf.nn.embedding_lookup(word_embeddings, u_inputs)\n",
    "    i_inputs = tf.nn.embedding_lookup(word_embeddings, i_inputs)\n",
    "    \n",
    "    user_conv1 = tf.layers.conv1d(\n",
    "        u_inputs,\n",
    "        filters,\n",
    "        kernel_size,\n",
    "        use_bias=True,\n",
    "        activation=tf.nn.tanh,\n",
    "        name=\"user_conv\")\n",
    "\n",
    "    item_conv1 = tf.layers.conv1d(\n",
    "        i_inputs,\n",
    "        filters,\n",
    "        kernel_size,\n",
    "        use_bias=True,\n",
    "        activation=tf.nn.tanh,\n",
    "        name=\"item_conv\")\n",
    "\n",
    "    user_max_pool1 = tf.layers.max_pooling1d(user_conv1, 2, 1)\n",
    "    item_max_pool1 = tf.layers.max_pooling1d(item_conv1, 2, 1)\n",
    "\n",
    "    user_flat = tf.layers.flatten(user_max_pool1)\n",
    "    item_flat = tf.layers.flatten(item_max_pool1)\n",
    "\n",
    "    user_dense = tf.layers.dense(user_flat, 64, activation=tf.nn.relu)\n",
    "    item_dense = tf.layers.dense(item_flat, 64, activation=tf.nn.relu)\n",
    "    \n",
    "    predictions = tf.reduce_sum( tf.multiply( user_dense, item_dense ), 1, keepdims=True )\n",
    "    \n",
    "    output = {\n",
    "        \"rating\": predictions,\n",
    "        \"user_review_embedding\": user_flat,\n",
    "        \"item_review_embedding\": item_flat\n",
    "    }\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "\n",
    "    # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "    loss = tf.losses.mean_squared_error(labels, predictions)\n",
    "\n",
    "    # Configure the Training Op (for TRAIN mode)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "        train_op = optimizer.minimize(\n",
    "            loss=loss,\n",
    "            global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "    # Add evaluation metrics (for EVAL mode)\n",
    "    eval_metric_ops = {\n",
    "        \"mean square error\": tf.metrics.mean_squared_error(\n",
    "            labels=labels, predictions=predictions)\n",
    "    }\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "truncate_len = 800\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_truncate_fn(trunc_len):\n",
    "    def truncate_fn(user, item, rating):\n",
    "        return user[:trunc_len], item[:trunc_len], rating\n",
    "    return truncate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_fn(user, item, rating):\n",
    "    user = tf.string_split(user)\n",
    "    item = tf.string_split(item)\n",
    "    return user.values, item.values, rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_fn(record):\n",
    "    features = {\n",
    "            \"user_review\": tf.FixedLenSequenceFeature([], tf.string, allow_missing=True),\n",
    "            \"item_review\": tf.FixedLenSequenceFeature([], tf.string, allow_missing=True),\n",
    "            \"rating\": tf.FixedLenFeature([1], tf.float32)\n",
    "        }\n",
    "    parsed_features = tf.parse_single_example(record, features)\n",
    "    return parsed_features[\"user_review\"], parsed_features[\"item_review\"], parsed_features[\"rating\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_iterator(loc, batch_size, max_len, pad_value):\n",
    "    dataset = tf.data.TFRecordDataset(loc)\n",
    "    dataset = dataset.prefetch(batch_size)\n",
    "    dataset = dataset.map(parse_fn, num_parallel_calls=batch_size)\n",
    "    dataset = dataset.map(split_fn, num_parallel_calls=batch_size)\n",
    "    dataset = dataset.map(get_truncate_fn(max_len), num_parallel_calls=batch_size)\n",
    "    dataset = dataset.padded_batch(batch_size, padded_shapes=([max_len], [max_len], [None]), padding_values=(pad_value, pad_value, 0.0))\n",
    "    dataset = dataset.shuffle(26352, reshuffle_each_iteration=False)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    return iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_input_fn():\n",
    "    train_dataset = get_dataset_iterator(\n",
    "        loc=\"data/train.tfrecords\",\n",
    "        batch_size=batch_size,\n",
    "        max_len=truncate_len,\n",
    "        pad_value=\"unk\")\n",
    "    nex = train_dataset.get_next()\n",
    "    return {\"user_input\": nex[0], \"item_input\": nex[1]}, tf.cast(nex[2], tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_input_fn():\n",
    "    test_dataset = get_dataset_iterator(\n",
    "        loc=\"data/test.tfrecords\",\n",
    "        batch_size=batch_size,\n",
    "        max_len=truncate_len,\n",
    "        pad_value=\"unk\"\n",
    "    )\n",
    "    nex = test_dataset.get_next()\n",
    "    return (nex[0], nex[1]), nex[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'dlprof/1525550398', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f98822924a8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "model_dir = \"dlprof/\" + str(int(time.time()))\n",
    "scoring_function = tf.estimator.Estimator(\n",
    "    model_fn=model_fn,\n",
    "    model_dir=model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into dlprof/1525550398/model.ckpt.\n",
      "INFO:tensorflow:loss = 13.210334, step = 0\n",
      "INFO:tensorflow:global_step/sec: 71.8545\n",
      "INFO:tensorflow:loss = 1.9414926, step = 100 (1.393 sec)\n",
      "INFO:tensorflow:global_step/sec: 75.6699\n",
      "INFO:tensorflow:loss = 1.089664, step = 200 (1.321 sec)\n",
      "INFO:tensorflow:global_step/sec: 71.5313\n",
      "INFO:tensorflow:loss = 0.7747723, step = 300 (1.398 sec)\n",
      "INFO:tensorflow:global_step/sec: 74.0304\n",
      "INFO:tensorflow:loss = 1.2600307, step = 400 (1.351 sec)\n",
      "INFO:tensorflow:global_step/sec: 70.2353\n",
      "INFO:tensorflow:loss = 2.2621274, step = 500 (1.425 sec)\n",
      "INFO:tensorflow:global_step/sec: 71.1435\n",
      "INFO:tensorflow:loss = 1.5349941, step = 600 (1.404 sec)\n",
      "INFO:tensorflow:global_step/sec: 74.3001\n",
      "INFO:tensorflow:loss = 1.5539387, step = 700 (1.347 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.9641\n",
      "INFO:tensorflow:loss = 0.52013415, step = 800 (1.250 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 824 into dlprof/1525550398/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.7256149.\n",
      "ran 31.494051694869995 seconds\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "scoring_function.train(input_fn=train_input_fn)\n",
    "e = time.time()\n",
    "print(\"ran {} seconds\".format(e - s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  serving_input_fn():\n",
    "    input_placeholder = tf.placeholder(tf.string, shape=None)\n",
    "    receiver_tensors = {\n",
    "        \"user_input\": tf.placeholder(tf.string, shape=None),\n",
    "        \"item_input\": tf.placeholder(tf.string, shape=None)\n",
    "    }\n",
    "    features = tf.parse_single_example(\n",
    "        input_placeholder,\n",
    "        {\n",
    "            \"user_input\": tf.FixedLenSequenceFeature([truncate_len], tf.string, allow_missing=True),\n",
    "            \"item_input\": tf.FixedLenSequenceFeature([truncate_len], tf.string, allow_missing=True)\n",
    "        }\n",
    "    )\n",
    "    return tf.estimator.export.ServingInputReceiver(features, receiver_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "export_outputs must be a dict and not<class 'NoneType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-ca57f44a7307>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m scoring_function.export_savedmodel(\n\u001b[1;32m      2\u001b[0m     \u001b[0mmodel_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mserving_input_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m )\n",
      "\u001b[0;32m~/dlprof/venv/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36mexport_savedmodel\u001b[0;34m(self, export_dir_base, serving_input_receiver_fn, assets_extra, as_text, checkpoint_path, strip_default_attrs)\u001b[0m\n\u001b[1;32m    617\u001b[0m           \u001b[0mserving_input_receiver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreceiver_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m           \u001b[0mestimator_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 619\u001b[0;31m           serving_input_receiver.receiver_tensors_alternatives)\n\u001b[0m\u001b[1;32m    620\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dlprof/venv/lib/python3.6/site-packages/tensorflow/python/estimator/export/export.py\u001b[0m in \u001b[0;36mbuild_all_signature_defs\u001b[0;34m(receiver_tensors, export_outputs, receiver_tensors_alternatives)\u001b[0m\n\u001b[1;32m    252\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mexport_outputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexport_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m     raise ValueError('export_outputs must be a dict and not'\n\u001b[0;32m--> 254\u001b[0;31m                      '{}'.format(type(export_outputs)))\n\u001b[0m\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m   \u001b[0msignature_def_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: export_outputs must be a dict and not<class 'NoneType'>"
     ]
    }
   ],
   "source": [
    "scoring_function.export_savedmodel(\n",
    "    model_dir,\n",
    "    serving_input_fn\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
