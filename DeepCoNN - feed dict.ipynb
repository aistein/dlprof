{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ecbm6040-as5281/miniconda3/envs/py36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import tensorflow as tf\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "truncate_len = 800\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_movie_lover = \"\"\"i really love action movies they are my favorite kind of movie because i love to watch the good guys\n",
    "win. some of my favorite actors are jet li and jackie chan because they were the last great actors who\n",
    "actually knew how to fight. modern action movie actors are just pretty faces and the editors swap camera\n",
    "angles when supposed hits make contact\"\"\"\n",
    "action_movie_hater = \"\"\"I really hate action movies. jackie chan and jet li are the worst. their old fashioned\n",
    "special effects, if you can even call them that, are out dated and boring. why are people even still paying\n",
    "for them to make movies\"\"\"\n",
    "item = \"\"\"this movie is great because of the way the strong, clever, hero saves the day at the end. as always\n",
    "jackie chans action directing and stunts are amazing, i'm so glad he doesn't use a stunt double liek\n",
    "some other actors that don't need to be mentioned\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch_Dataset(object):\n",
    "    def __init__(self, user_review_location, item_review_location, rating_location, pad_length, pad_value, batch_size):\n",
    "        user_review_list = [line.split()[:truncate_len] for line in self._get_lines(user_review_location)]\n",
    "        self.user_review_list = self._pad_if_necessary(user_review_list, pad_value, pad_length)\n",
    "        \n",
    "        item_review_list = [line.split()[:truncate_len] for line in self._get_lines(item_review_location)]\n",
    "        self.item_review_list = self._pad_if_necessary(item_review_list, pad_value, pad_length)\n",
    "        \n",
    "        self.ratings = [float(rating) for rating in self._get_lines(rating_location)]\n",
    "        \n",
    "        self.current = 0\n",
    "        self.batch_size = batch_size\n",
    "        self.stop_len = len(self.ratings)\n",
    "    \n",
    "    def _get_lines(self, fname):\n",
    "        with open(fname, \"rt\") as data:\n",
    "            return data.read().splitlines()\n",
    "        \n",
    "    def _pad_if_necessary(self, list_of_lists, pad_value, pad_length):\n",
    "        return [np.array(line + [pad_value] * (pad_length - len(line))) for line in list_of_lists]\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.current + self.batch_size > self.stop_len:\n",
    "            self.current = 0\n",
    "            self.prev = 0\n",
    "            raise StopIteration\n",
    "        else:\n",
    "            self.prev = self.current\n",
    "            self.current += self.batch_size\n",
    "            return (np.array(self.user_review_list[self.prev: self.current]),\n",
    "                    np.array(self.item_review_list[self.prev: self.current]),\n",
    "                    np.array(self.ratings[self.prev: self.current]).reshape(self.batch_size, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ecbm6040-as5281/miniconda3/envs/py36/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "WARNING:tensorflow:From /home/ecbm6040-as5281/miniconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n",
      "WARNING:tensorflow:From <ipython-input-5-01e910fde803>:60: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "2018-04-15 22:54:31.248233: epoch 101, loss 1.49\n",
      "2018-04-15 22:54:34.302774: epoch 202, loss 2.01\n",
      "2018-04-15 22:54:37.369570: epoch 303, loss 0.66\n",
      "2018-04-15 22:54:40.413698: epoch 404, loss 1.53\n",
      "2018-04-15 22:54:43.445498: epoch 505, loss 0.97\n",
      "2018-04-15 22:54:46.480845: epoch 606, loss 1.30\n",
      "2018-04-15 22:54:49.541625: epoch 707, loss 0.89\n",
      "2018-04-15 22:54:52.621426: epoch 808, loss 1.02\n",
      "ran 45.2598602771759 seconds\n"
     ]
    }
   ],
   "source": [
    "emb_size = 50\n",
    "filters = 10\n",
    "kernel_size = 3\n",
    "n_epochs = 824\n",
    "\n",
    "s = time.time()\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with open(\"data/dictionary.pkl\", \"rb\") as f:\n",
    "    dictionary = pkl.load(f)\n",
    "\n",
    "values = list(range(len(dictionary)))\n",
    "keys = list(dictionary)\n",
    "\n",
    "table = tf.contrib.lookup.HashTable(\n",
    "  tf.contrib.lookup.KeyValueTensorInitializer(keys, values), -1\n",
    ")\n",
    "\n",
    "word_embeddings = tf.get_variable(\n",
    "    \"word_embeddings\",\n",
    "    shape=[len(dictionary), emb_size]\n",
    ")\n",
    "\n",
    "u_inputs = tf.placeholder(tf.string, (batch_size, truncate_len), name=\"user_inputs\")\n",
    "i_inputs = tf.placeholder(tf.string, (batch_size, truncate_len), name=\"item_inputs\")\n",
    "ratings_input = tf.placeholder(tf.float64, (batch_size, 1), name=\"ratings\")\n",
    "\n",
    "u_inputs_indices = table.lookup(u_inputs)\n",
    "i_inputs_indices = table.lookup(i_inputs)\n",
    "\n",
    "u_inputs_embedded = tf.nn.embedding_lookup(word_embeddings, u_inputs_indices)\n",
    "i_inputs_embedded = tf.nn.embedding_lookup(word_embeddings, i_inputs_indices)\n",
    "\n",
    "user_conv1 = tf.layers.conv1d(\n",
    "    u_inputs_embedded,\n",
    "    filters,\n",
    "    kernel_size,\n",
    "    use_bias=True,\n",
    "    activation=tf.nn.tanh,\n",
    "    name=\"user_conv\")\n",
    "\n",
    "item_conv1 = tf.layers.conv1d(\n",
    "    i_inputs_embedded,\n",
    "    filters,\n",
    "    kernel_size,\n",
    "    use_bias=True,\n",
    "    activation=tf.nn.tanh,\n",
    "    name=\"item_conv\")\n",
    "\n",
    "user_max_pool1 = tf.layers.max_pooling1d(user_conv1, 2, 1)\n",
    "item_max_pool1 = tf.layers.max_pooling1d(item_conv1, 2, 1)\n",
    "\n",
    "user_flat = tf.layers.flatten(user_max_pool1)\n",
    "item_flat = tf.layers.flatten(item_max_pool1)\n",
    "\n",
    "user_dense = tf.layers.dense(user_flat, 64, activation=tf.nn.relu)\n",
    "item_dense = tf.layers.dense(item_flat, 64, activation=tf.nn.relu)\n",
    "\n",
    "predictions = tf.reduce_sum( tf.multiply( user_dense, item_dense ), 1, keep_dims=True )\n",
    "\n",
    "loss = tf.losses.mean_squared_error(ratings_input, predictions)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "\n",
    "train_op = optimizer.minimize(\n",
    "    loss=loss)\n",
    "\n",
    "vars_init = tf.global_variables_initializer()\n",
    "tables_init = tf.tables_initializer()\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(vars_init)\n",
    "    sess.run(tables_init)\n",
    "    \n",
    "    dataset = Batch_Dataset(\"data/train_u_reviews.txt\",\n",
    "                            \"data/train_i_reviews.txt\",\n",
    "                            \"data/train_ratings.txt\",\n",
    "                            truncate_len,\n",
    "                            \"unk\",\n",
    "                            batch_size)\n",
    "    \n",
    "    i = 0\n",
    "    for user_batch, item_batch, rating_batch in dataset:\n",
    "        \n",
    "        i += 1\n",
    "        \n",
    "        _, l = sess.run([train_op, loss], feed_dict={\n",
    "            u_inputs: user_batch,\n",
    "            i_inputs: item_batch,\n",
    "            ratings_input: rating_batch\n",
    "        })\n",
    "        if i % 101 == 0:\n",
    "            print(\"{}: epoch {}, loss {:.2f}\".format(str(datetime.datetime.now()), i, l))\n",
    "            \n",
    "e = time.time()\n",
    "\n",
    "print(\"ran {} seconds\".format(e - s))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
