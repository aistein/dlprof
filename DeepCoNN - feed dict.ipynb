{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/venv/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import tensorflow as tf\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "truncate_len = 800\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_movie_lover = \"\"\"i really love action movies they are my favorite kind of movie because i love to watch the good guys\n",
    "win. some of my favorite actors are jet li and jackie chan because they were the last great actors who\n",
    "actually knew how to fight. modern action movie actors are just pretty faces and the editors swap camera\n",
    "angles when supposed hits make contact\"\"\"\n",
    "action_movie_hater = \"\"\"I really hate action movies. jackie chan and jet li are the worst. their old fashioned\n",
    "special effects, if you can even call them that, are out dated and boring. why are people even still paying\n",
    "for them to make movies\"\"\"\n",
    "item = \"\"\"this movie is great because of the way the strong, clever, hero saves the day at the end. as always\n",
    "jackie chans action directing and stunts are amazing, i'm so glad he doesn't use a stunt double liek\n",
    "some other actors that don't need to be mentioned\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lines(fname):\n",
    "    with open(fname, \"rt\") as data:\n",
    "        return data.read().splitlines()\n",
    "user_review_list = [line.split()[:truncate_len] for line in get_lines(\"data/train_u_reviews.txt\")]\n",
    "item_review_list = [line.split()[:truncate_len] for line in get_lines(\"data/train_i_reviews.txt\")]\n",
    "ratings = [float(rating) for rating in get_lines(\"data/train_ratings.txt\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_if_necessary(list_of_lists, pad_value, pad_length):\n",
    "    return [np.array(line + [pad_value] * (pad_length - len(line))) for line in list_of_lists]\n",
    "users = pad_if_necessary(user_review_list, \"unk\", truncate_len)\n",
    "items = pad_if_necessary(item_review_list, \"unk\", truncate_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(users, items, ratings, batch_size):\n",
    "    indices = np.random.choice(np.arange(len(ratings)), size=batch_size, replace=False)\n",
    "    return np.array(users)[indices], np.array(items)[indices], np.array(ratings)[indices].reshape(batch_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-15 20:34:17.247294: epoch 0, loss 18.09\n",
      "2018-04-15 20:35:59.422980: epoch 25, loss 12.68\n",
      "2018-04-15 20:37:41.322417: epoch 50, loss 5.87\n",
      "2018-04-15 20:39:23.486592: epoch 75, loss 2.62\n",
      "2018-04-15 20:41:05.507786: epoch 100, loss 2.22\n",
      "2018-04-15 20:42:47.578417: epoch 125, loss 1.89\n",
      "2018-04-15 20:44:29.593515: epoch 150, loss 1.98\n"
     ]
    }
   ],
   "source": [
    "emb_size = 50\n",
    "filters = 10\n",
    "kernel_size = 3\n",
    "n_epochs = 824\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with open(\"data/dictionary.pkl\", \"rb\") as f:\n",
    "    dictionary = pkl.load(f)\n",
    "\n",
    "values = list(range(len(dictionary)))\n",
    "keys = list(dictionary)\n",
    "\n",
    "table = tf.contrib.lookup.HashTable(\n",
    "  tf.contrib.lookup.KeyValueTensorInitializer(keys, values), -1\n",
    ")\n",
    "\n",
    "word_embeddings = tf.get_variable(\n",
    "    \"word_embeddings\",\n",
    "    shape=[len(dictionary), emb_size]\n",
    ")\n",
    "\n",
    "u_inputs = tf.placeholder(tf.string, (batch_size, truncate_len), name=\"user_inputs\")\n",
    "i_inputs = tf.placeholder(tf.string, (batch_size, truncate_len), name=\"item_inputs\")\n",
    "ratings_input = tf.placeholder(tf.float64, (batch_size, 1), name=\"ratings\")\n",
    "\n",
    "u_inputs_indices = table.lookup(u_inputs)\n",
    "i_inputs_indices = table.lookup(i_inputs)\n",
    "\n",
    "u_inputs_embedded = tf.nn.embedding_lookup(word_embeddings, u_inputs_indices)\n",
    "i_inputs_embedded = tf.nn.embedding_lookup(word_embeddings, i_inputs_indices)\n",
    "\n",
    "user_conv1 = tf.layers.conv1d(\n",
    "    u_inputs_embedded,\n",
    "    filters,\n",
    "    kernel_size,\n",
    "    use_bias=True,\n",
    "    activation=tf.nn.tanh,\n",
    "    name=\"user_conv\")\n",
    "\n",
    "item_conv1 = tf.layers.conv1d(\n",
    "    i_inputs_embedded,\n",
    "    filters,\n",
    "    kernel_size,\n",
    "    use_bias=True,\n",
    "    activation=tf.nn.tanh,\n",
    "    name=\"item_conv\")\n",
    "\n",
    "user_max_pool1 = tf.layers.max_pooling1d(user_conv1, 2, 1)\n",
    "item_max_pool1 = tf.layers.max_pooling1d(item_conv1, 2, 1)\n",
    "\n",
    "user_flat = tf.layers.flatten(user_max_pool1)\n",
    "item_flat = tf.layers.flatten(item_max_pool1)\n",
    "\n",
    "user_dense = tf.layers.dense(user_flat, 64, activation=tf.nn.relu)\n",
    "item_dense = tf.layers.dense(item_flat, 64, activation=tf.nn.relu)\n",
    "\n",
    "predictions = tf.reduce_sum( tf.multiply( user_dense, item_dense ), 1, keep_dims=True )\n",
    "\n",
    "loss = tf.losses.mean_squared_error(ratings_input, predictions)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "\n",
    "train_op = optimizer.minimize(\n",
    "    loss=loss)\n",
    "\n",
    "vars_init = tf.global_variables_initializer()\n",
    "tables_init = tf.tables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(vars_init)\n",
    "    sess.run(tables_init)\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    for i in range(n_epochs):\n",
    "        \n",
    "        user_batch, item_batch, rating_batch = get_batch(users, items, ratings, batch_size)\n",
    "        \n",
    "        _, l = sess.run([train_op, loss], feed_dict={\n",
    "            u_inputs: user_batch,\n",
    "            i_inputs: item_batch,\n",
    "            ratings_input: rating_batch\n",
    "        })\n",
    "        if i % 101 == 0:\n",
    "            print(\"{}: epoch {}, loss {:.2f}\".format(str(datetime.datetime.now()), i, l))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
